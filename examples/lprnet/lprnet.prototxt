name: "lprnet"
layer {
    name: "data"
    type: "HDF5Data"
    top: "data"
    top: "label"
    include {
        phase: TRAIN
    }
    hdf5_data_param {
        source: "./data/plate/lpr/plate_trainning.list"
        batch_size: 128 
    }
}
#w:94*24*1
layer {
    name: "data"
    type: "HDF5Data"
    top: "data"
    top: "label"
    include {
        phase: TEST
    }
    hdf5_data_param {
        source: "./data/plate/lpr/plate_testing.list"
        batch_size: 128 
    }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
#w*h:48*16
layer {
	bottom: "conv0"
	top: "conv0"
	name: "bn_conv0"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer{
    bottom:"conv0"
    top: "conv0"
    name: "bn_conv0"
    type: "BatchNorm"
    batch_norm_param{
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
layer {
	bottom: "conv0"
	top: "conv0"
	name: "scale_conv0"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu0"
  type: "ReLU"
  bottom: "conv0"
  top: "conv0"
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
### SMALL BASIC BLOCK ONE starts!!!###
layer {
  name: "sbb1"
  type: "Convolution"
  bottom: "pool0"
  top: "sbb1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convb_sbb1"
  type: "Convolution"
  bottom: "sbb1"
  top: "sbb1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 16
    kernel_w: 3
    kernel_h: 1
    stride: 1
    pad_w: 1
    pad_h: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convc_sbb1"
  type: "Convolution"
  bottom: "sbb1"
  top: "sbb1"
  convolution_param {
    num_output: 16
    kernel_w: 1
    kernel_h: 3
    stride: 1
    pad_w: 0
    pad_h: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convd_sbb1"
  type: "Convolution"
  bottom: "sbb1"
  top: "sbb1"
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
##SMALL BASIC BLOCK One ends!!!###
layer {
	bottom: "sbb1"
	top: "conv1"
	name: "conv1"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer{
    bottom:"sbb1"
    top: "conv1"
    name: "conv1"
    type: "BatchNorm"
    batch_norm_param{
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
layer {
	bottom: "conv1"
	top: "conv1"
	name: "scale_conv1"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "conv1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride_w: 2
    stride_h: 1
    pad_w: 0
    pad_h: 1
  }
}
#w*h:47*24
### SMALL BASIC BLOCK TWO starts!!!###
layer {
  name: "sbb2"
  type: "Convolution"
  bottom: "conv1"
  top: "sbb2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convb_sbb2"
  type: "Convolution"
  bottom: "sbb2"
  top: "sbb2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_w: 3
    kernel_h: 1
    stride: 1
    pad_w: 1
    pad_h: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convc_sbb2"
  type: "Convolution"
  bottom: "sbb2"
  top: "sbb2"
  convolution_param {
    num_output: 64
    kernel_w: 1
    kernel_h: 3
    stride: 1
    pad_w: 0
    pad_h: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convd_sbb2"
  type: "Convolution"
  bottom: "sbb2"
  top: "sbb2"
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
##SMALL BASIC BLOCK TWO ends!!!###
layer {
	bottom: "sbb2"
	top: "conv2"
	name: "conv2"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer{
    bottom:"sbb2"
    top: "conv2"
    name: "conv2"
    type: "BatchNorm"
    batch_norm_param{
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
layer {
	bottom: "conv2"
	top: "conv2"
	name: "scale_conv2"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
### SMALL BASIC BLOCK THREE starts!!!###
layer {
  name: "sbb3"
  type: "Convolution"
  bottom: "conv2"
  top: "sbb3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convb_sbb3"
  type: "Convolution"
  bottom: "sbb3"
  top: "sbb3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_w: 3
    kernel_h: 1
    stride: 1
    pad_w: 1
    pad_h: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convc_sbb3"
  type: "Convolution"
  bottom: "sbb3"
  top: "sbb3"
  convolution_param {
    num_output: 64
    kernel_w: 1
    kernel_h: 3
    stride: 1
    pad_w: 0
    pad_h: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "convd_sbb3"
  type: "Convolution"
  bottom: "sbb3"
  top: "sbb3"
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
##SMALL BASIC BLOCK THREE ends!!!###
layer {
	bottom: "sbb3"
	top: "conv3"
	name: "conv3"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer{
    bottom:"sbb3"
    top: "conv3"
    name: "conv3"
    type: "BatchNorm"
    batch_norm_param{
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
layer {
	bottom: "conv3"
	top: "conv3"
	name: "scale_conv3"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv3"
  top: "conv3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride_w: 2
    stride_h: 1
    pad_w: 0
    pad_h: 1
  }
}
layer {
  name: "drop0"
  type: "Dropout"
  bottom: "conv3"
  top: "conv3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
#w*h:24*24
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_w: 4
    kernel_h: 1
    stride: 1
    pad_w: 2
    pad_h: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
	bottom: "conv4"
	top: "conv4"
	name: "bn_conv4"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer {
    bottom: "conv4"
    top: "conv4"
    name: "bn_conv4"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
layer {
	bottom: "conv4"
	top: "conv4"
	name: "scale_conv4"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
### Start of conv5
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 74
    kernel_w: 1
    kernel_h: 13
    stride: 1
    pad_w: 0
    pad_h: 6
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
#24*24
layer {
	bottom: "conv5"
	top: "conv5"
	name: "bn_conv5"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer{
    bottom:"conv5"
    top: "conv5"
    name: "bn_conv5"
    type: "BatchNorm"
    batch_norm_param{
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
layer {
	bottom: "conv5"
	top: "conv5"
	name: "scale_conv5"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
###Average Pooling of INPUTS##
layer {
  name: "apool0"
  type: "Pooling"
  bottom: "data"
  top: "apool0"
  pooling_param {
    pool: AVE
    kernel_w: 4
    kernel_h: 1
    stride_w: 4
    stride_h: 1
    pad_w: 1
    pad_h: 0
  }
}
layer {
	bottom: "apool0"
	top: "apool0"
	name: "norm_apool0"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer {
	bottom: "apool0"
	top: "apool0"
	name: "norm_apool0"
	type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
###Average Pooling of sbb1##
layer {
  name: "apool1"
  type: "Pooling"
  bottom: "sbb1"
  top: "apool1"
  pooling_param {
    pool: AVE
    kernel_w: 4
    kernel_h: 1
    stride_w: 4
    stride_h: 1
    pad_w: 1
    pad_h: 0
  }
}
layer {
	bottom: "apool1"
	top: "apool1"
	name: "norm_apool1"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer {
	bottom: "apool1"
	top: "apool1"
	name: "norm_apool1"
	type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
###Average Pooling of sbb2##
layer {
  name: "apool2"
  type: "Pooling"
  bottom: "sbb2"
  top: "apool2"
  pooling_param {
    pool: AVE
    kernel_w: 2
    kernel_h: 1
    stride_w: 2
    stride_h: 1
    pad_w: 0
    pad_h: 0
  }
}
layer {
	bottom: "apool2"
	top: "apool2"
	name: "norm_apool2"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
    include {
        phase: TRAIN
    }
}
layer {
	bottom: "apool2"
	top: "apool2"
	name: "norm_apool2"
	type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
    include{
        phase: TEST
    }
}
### End of Pooling of sbb2"
layer {
  name: "concat1"
  bottom: "apool0"
  bottom: "apool1"
  top: "concat1"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concat2"
  bottom: "concat1"
  bottom: "apool2"
  top: "concat2"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "concat3"
  bottom: "conv5"
  bottom: "concat2"
  top: "concat3"
  type: "Concat"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv6"
  type: "Convolution"
  bottom: "concat3"
  top: "conv6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 74
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "permute_hw"
  type: "Permute"
  bottom: "conv6"
  top: "conv6"
  permute_param {
    order: 0
    order: 1
    order: 3
    order: 2
  }
}
layer {
  name: "reduction"
  type: "Reduction"
  bottom: "conv6"
  top: "conv6"
  reduction_param {
    axis: 3   #reduce the H dim
    operation: MEAN  # use mean
  }
}

layer {
  name: "permute_c6"
  type: "Permute"
  bottom: "conv6"
  top: "permute_c6"
  permute_param {
    order: 2
    order: 0
    order: 1
  }
}
layer {
	name: "ctc_loss"
	type: "CtcLoss"
	bottom: "permute_c6"
	bottom: "label"
	top: "ctc_loss"
	loss_weight: 1.0
	ctc_loss_param {
		blank_label: 73
		alphabet_size: 74
    	time_step: 24
	}
}

layer {
  name: "permute_fc"
  type: "Permute"
  bottom: "conv6"
  top: "permute_fc"
  permute_param {
    order: 0
    order: 2
    order: 1
  }
}
layer {
  name: "accuracy"
  type: "LabelsequenceAccuracy"
  bottom: "permute_fc"
  bottom: "label"
  top: "accuracy"
  labelsequence_accuracy_param {
    blank_label: 74
  }
}
